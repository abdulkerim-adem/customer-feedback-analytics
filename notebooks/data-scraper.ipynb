{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google_play_scraper import Sort, reviews_all, reviews\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define App Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_IDS = {\n",
    "    'CBE': 'com.combanketh.mobilebanking',\n",
    "    'BOA': 'com.boa.boaMobileBanking',\n",
    "    'DASHEN': 'com.dashen.dashensuperapp'         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Reviews Per App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_REVIEWS_PER_APP = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playstore Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayStoreScraper:\n",
    "    \"\"\"\n",
    "    A class to scrape app reviews from the Google Play Store.\n",
    "    \"\"\"\n",
    "    def __init__(self, app_ids_dict):\n",
    "        \"\"\"\n",
    "        Initializes the scraper with a dictionary of app names and their IDs.\n",
    "        Args:\n",
    "            app_ids_dict (dict): A dictionary where keys are bank names\n",
    "                                 and values are their Google Play app IDs.\n",
    "        \"\"\"\n",
    "        self.app_ids = app_ids_dict\n",
    "        self.reviews_data = []\n",
    "\n",
    "    def fetch_reviews_for_app(self, app_id, app_name, target_count=400, lang='en', country='us'):\n",
    "        \"\"\"\n",
    "        Fetches a target number of reviews for a single app.\n",
    "        Args:\n",
    "            app_id (str): The Google Play ID of the app.\n",
    "            app_name (str): The name of the bank/app.\n",
    "            target_count (int): The desired number of reviews.\n",
    "            lang (str): Language code for reviews.\n",
    "            country (str): Country code for the Play Store.\n",
    "        Returns:\n",
    "            list: A list of review dictionaries.\n",
    "        \"\"\"\n",
    "        print(f\"Fetching reviews for {app_name} ({app_id})...\")\n",
    "        try:\n",
    "            # Using 'reviews' which allows for 'count' parameter.\n",
    "            # 'reviews_all' fetches all, which might be too many initially.\n",
    "            \n",
    "            app_reviews = []\n",
    "            result, continuation_token = reviews(\n",
    "                app_id,\n",
    "                lang=lang,\n",
    "                country=country,\n",
    "                sort=Sort.NEWEST, # Or Sort.MOST_RELEVANT\n",
    "                count=target_count + 200, # Fetch a bit more to account for filtering, duplicates if any from source\n",
    "                filter_score_with=None # All scores\n",
    "            )\n",
    "            \n",
    "            if not result:\n",
    "                print(f\"No reviews found for {app_name} with current settings.\")\n",
    "                return []\n",
    "\n",
    "            for review in result:\n",
    "                self.reviews_data.append({\n",
    "                    'review_id': review.get('reviewId'),\n",
    "                    'user_name': review.get('userName'),\n",
    "                    'review': review.get('content'),\n",
    "                    'rating': review.get('score'),\n",
    "                    'date': review.get('at'), # This is a datetime object\n",
    "                    'bank': app_name,\n",
    "                    'source': 'Google Play Store' \n",
    "                })\n",
    "                if len(self.reviews_data) % 100 == 0:\n",
    "                    print(f\"Collected {len(self.reviews_data)} reviews for {app_name} so far...\")\n",
    "\n",
    "            print(f\"Successfully fetched {len(result)} reviews for {app_name}.\")\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching reviews for {app_id}: {e}\")\n",
    "        return result # Return the directly fetched reviews for this app\n",
    "\n",
    "    def fetch_all_reviews(self, target_per_app):\n",
    "        \"\"\"\n",
    "        Fetches reviews for all apps defined in app_ids.\n",
    "        Args:\n",
    "            target_per_app (int): Target number of reviews per app.\n",
    "        \"\"\"\n",
    "        all_scraped_reviews = []\n",
    "        for bank_name, app_id in self.app_ids.items():\n",
    "            print(f\"\\nStarting scraping for: {bank_name}\")\n",
    "\n",
    "            fetched_app_reviews, _ = reviews(\n",
    "                app_id,\n",
    "                lang='en',\n",
    "                country='et',     # Focusing on Ethiopian context\n",
    "                sort=Sort.NEWEST, # Get the most recent feedback\n",
    "                count=target_per_app + 50, # Fetch a bit more to be safe for the 400 target\n",
    "                filter_score_with=None # Get all ratings\n",
    "            )\n",
    "            \n",
    "            if fetched_app_reviews:\n",
    "                for review_obj in fetched_app_reviews[:target_per_app]: # Take top N to ensure we don't vastly overshoot\n",
    "                    all_scraped_reviews.append({\n",
    "                        'review': review_obj.get('content'),\n",
    "                        'rating': review_obj.get('score'),\n",
    "                        'date': review_obj.get('at'), # This is a datetime object\n",
    "                        'bank': bank_name,\n",
    "                        'source': 'Google Play Store' # As per dataset overview \n",
    "                    })\n",
    "                print(f\"Collected {len(all_scraped_reviews)} reviews so far (target for {bank_name}: {target_per_app}).\")\n",
    "            else:\n",
    "                print(f\"Could not fetch reviews for {bank_name} ({app_id}).\")\n",
    "            \n",
    "            time.sleep(2) # Basic courtesy delay\n",
    "\n",
    "        self.reviews_df = pd.DataFrame(all_scraped_reviews)\n",
    "        print(f\"\\nTotal reviews scraped: {len(self.reviews_df)}\")\n",
    "        return self.reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to preprocess the scraped app reviews.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the preprocessor with a DataFrame of reviews.\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the raw review data.\n",
    "        \"\"\"\n",
    "        self.df = df.copy() # Work on a copy\n",
    "\n",
    "    def normalize_dates(self, date_column='date', format='%Y-%m-%d'): # \n",
    "        \"\"\"\n",
    "        Normalizes the date column to YYYY-MM-DD format.\n",
    "        Args:\n",
    "            date_column (str): Name of the column containing date information.\n",
    "            format (str): The desired output format for the date.\n",
    "        \"\"\"\n",
    "        print(\"Normalizing dates...\")\n",
    "        if date_column in self.df.columns:\n",
    "            self.df[date_column] = pd.to_datetime(self.df[date_column]).dt.strftime(format)\n",
    "            print(f\"'{date_column}' normalized.\")\n",
    "        else:\n",
    "            print(f\"Warning: Date column '{date_column}' not found.\")\n",
    "        return self\n",
    "\n",
    "    def handle_missing_data(self): # \n",
    "        \"\"\"\n",
    "        Handles missing data in the DataFrame.\n",
    "        For reviews, if 'review' text or 'rating' is missing, it's often best to drop.\n",
    "        \"\"\"\n",
    "        print(\"Handling missing data...\")\n",
    "        initial_rows = len(self.df)\n",
    "        # Drop rows where essential information like 'review' or 'rating' is missing\n",
    "        self.df.dropna(subset=['review', 'rating'], inplace=True)\n",
    "        rows_dropped = initial_rows - len(self.df)\n",
    "        print(f\"Dropped {rows_dropped} rows due to missing 'review' or 'rating'.\")\n",
    "        return self\n",
    "\n",
    "    def remove_duplicates(self, subset_cols=None): # \n",
    "        \"\"\"\n",
    "        Removes duplicate reviews.\n",
    "        Args:\n",
    "            subset_cols (list, optional): List of column names to consider for identifying duplicates.\n",
    "                                          Defaults to ['review', 'bank', 'date']\n",
    "        \"\"\"\n",
    "        print(\"Removing duplicate reviews...\")\n",
    "        if subset_cols is None:\n",
    "            subset_cols = ['review', 'rating', 'date', 'bank']\n",
    "        \n",
    "        initial_rows = len(self.df)\n",
    "        self.df.drop_duplicates(subset=subset_cols, keep='first', inplace=True)\n",
    "        rows_dropped = initial_rows - len(self.df)\n",
    "        print(f\"Dropped {rows_dropped} duplicate rows.\")\n",
    "        return self\n",
    "        \n",
    "    def ensure_columns(self):\n",
    "        \"\"\"Ensures the DataFrame has the required columns: review, rating, date, bank, source. \"\"\"\n",
    "        print(\"Ensuring required columns...\")\n",
    "        required_cols = ['review', 'rating', 'date', 'bank', 'source']\n",
    "        # Select and reorder\n",
    "        current_cols = [col for col in required_cols if col in self.df.columns]\n",
    "        self.df = self.df[current_cols]\n",
    "        \n",
    "        # Check if all required columns are present after selection\n",
    "        missing_cols = [col for col in required_cols if col not in self.df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: The following required columns are still missing: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"All required columns are present.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Runs the full preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Starting Preprocessing ---\")\n",
    "        self.handle_missing_data()\n",
    "        self.remove_duplicates() # Consider which columns define a unique review\n",
    "        self.normalize_dates()\n",
    "        self.ensure_columns() # Ensure final columns as per \n",
    "        print(\"--- Preprocessing Complete ---\")\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for: CBE\n",
      "Collected 400 reviews so far (target for CBE: 400).\n",
      "\n",
      "Starting scraping for: BOA\n",
      "Collected 800 reviews so far (target for BOA: 400).\n",
      "\n",
      "Starting scraping for: DASHEN\n",
      "Collected 1200 reviews so far (target for DASHEN: 400).\n",
      "\n",
      "Total reviews scraped: 1200\n"
     ]
    }
   ],
   "source": [
    "scraper = PlayStoreScraper(APP_IDS)\n",
    "raw_reviews_df = scraper.fetch_all_reviews(target_per_app=TARGET_REVIEWS_PER_APP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and save the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of raw_reviews_df: (1200, 5)\n",
      "                                              review  rating  \\\n",
      "0                         So bad now and hard to use       5   \n",
      "1  it is so amazing app. but, it is better to upd...       5   \n",
      "2                                         v.good app       4   \n",
      "3                                      very good app       1   \n",
      "4           Very amazing app indeed. I'm enjoying it       5   \n",
      "\n",
      "                 date bank             source  \n",
      "0 2025-06-09 18:31:56  CBE  Google Play Store  \n",
      "1 2025-06-09 16:20:06  CBE  Google Play Store  \n",
      "2 2025-06-09 11:49:09  CBE  Google Play Store  \n",
      "3 2025-06-09 01:24:23  CBE  Google Play Store  \n",
      "4 2025-06-08 21:52:23  CBE  Google Play Store  \n",
      "\n",
      "Review counts per bank (raw):\n",
      "bank\n",
      "CBE       400\n",
      "BOA       400\n",
      "DASHEN    400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Starting Preprocessing ---\n",
      "Handling missing data...\n",
      "Dropped 0 rows due to missing 'review' or 'rating'.\n",
      "Removing duplicate reviews...\n",
      "Dropped 0 duplicate rows.\n",
      "Normalizing dates...\n",
      "'date' normalized.\n",
      "Ensuring required columns...\n",
      "All required columns are present.\n",
      "--- Preprocessing Complete ---\n",
      "\n",
      "Shape of cleaned_df: (1200, 5)\n",
      "                                              review  rating        date bank  \\\n",
      "0                         So bad now and hard to use       5  2025-06-09  CBE   \n",
      "1  it is so amazing app. but, it is better to upd...       5  2025-06-09  CBE   \n",
      "2                                         v.good app       4  2025-06-09  CBE   \n",
      "3                                      very good app       1  2025-06-09  CBE   \n",
      "4           Very amazing app indeed. I'm enjoying it       5  2025-06-08  CBE   \n",
      "\n",
      "              source  \n",
      "0  Google Play Store  \n",
      "1  Google Play Store  \n",
      "2  Google Play Store  \n",
      "3  Google Play Store  \n",
      "4  Google Play Store  \n",
      "\n",
      "Review counts per bank (cleaned):\n",
      "bank\n",
      "CBE       400\n",
      "BOA       400\n",
      "DASHEN    400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Overall missing data percentage after cleaning: 0.00%\n",
      "\n",
      "Cleaned data saved to ../data/cleaned_play_store_reviews.csv\n",
      "\n",
      "KPI Check - Total reviews collected: 1200\n",
      "Reviews for CBE: 400\n",
      "Reviews for BOA: 400\n",
      "Reviews for DASHEN: 400\n"
     ]
    }
   ],
   "source": [
    "# Display some info about scraped data\n",
    "if not raw_reviews_df.empty:\n",
    "    print(f\"\\nShape of raw_reviews_df: {raw_reviews_df.shape}\")\n",
    "    print(raw_reviews_df.head())\n",
    "    print(\"\\nReview counts per bank (raw):\")\n",
    "    print(raw_reviews_df['bank'].value_counts())\n",
    "\n",
    "    # 2. Preprocess Data\n",
    "    preprocessor = ReviewPreprocessor(raw_reviews_df)\n",
    "    cleaned_df = preprocessor.preprocess() # \n",
    "\n",
    "    print(f\"\\nShape of cleaned_df: {cleaned_df.shape}\")\n",
    "    print(cleaned_df.head())\n",
    "    print(\"\\nReview counts per bank (cleaned):\")\n",
    "    print(cleaned_df['bank'].value_counts())\n",
    "\n",
    "    # Check for missing data percentage\n",
    "    missing_percentage = (cleaned_df.isnull().sum().sum() / (len(cleaned_df) * len(cleaned_df.columns))) * 100\n",
    "    print(f\"\\nOverall missing data percentage after cleaning: {missing_percentage:.2f}%\") # Aim for <5% \n",
    "\n",
    "    # 3. Save to CSV \n",
    "    output_csv_path = '../data/cleaned_play_store_reviews.csv' # Make sure 'data' directory exists\n",
    "    # Create data directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    \n",
    "    cleaned_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nCleaned data saved to {output_csv_path}\")\n",
    "    \n",
    "    # KPI Checks:\n",
    "    print(f\"\\nKPI Check - Total reviews collected: {len(cleaned_df)}\") # Should be 1200+ \n",
    "    for bank_name in APP_IDS.keys():\n",
    "        count = len(cleaned_df[cleaned_df['bank'] == bank_name])\n",
    "        print(f\"Reviews for {bank_name}: {count}\") # Should be 400+ per bank \n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Scraping did not yield any data. Please check APP_IDS and network connectivity.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
