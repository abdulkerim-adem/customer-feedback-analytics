{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform sentiment analysis using a Hugging Face model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        \"\"\"\n",
    "        Initializes the sentiment analysis pipeline.\n",
    "        The challenge recommends this model.\n",
    "        \"\"\"\n",
    "        print(f\"Loading sentiment model: {model_name}...\")\n",
    "        # Using device=0 will use GPU if available, -1 for CPU\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=-1)\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "    def analyze(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Applies sentiment analysis to a DataFrame column.\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "            text_column (str): The column containing text to analyze.\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with added 'sentiment_label' and 'sentiment_score' columns.\n",
    "        \"\"\"\n",
    "        print(\"Applying sentiment analysis...\")\n",
    "        # The pipeline returns a list of dictionaries like {'label': 'POSITIVE', 'score': 0.999}\n",
    "        # We need to handle potential long texts by truncating them for the model\n",
    "        sentiments = self.sentiment_pipeline(df[text_column].fillna('').tolist(), truncation=True)\n",
    "        \n",
    "        # Extract labels and scores\n",
    "        df['sentiment_label'] = [s['label'] for s in sentiments]\n",
    "        df['sentiment_score'] = [s['score'] for s in sentiments]\n",
    "        print(\"Sentiment analysis complete.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThematicAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ThematicAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform thematic analysis using NLP techniques.\n",
    "    \"\"\"\n",
    "    def __init__(self, stop_words='english'):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 3), stop_words=stop_words)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Lemmatizes text and removes stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        doc = nlp(text.lower())\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    def extract_keywords(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Extracts top keywords for each bank using TF-IDF.\n",
    "        \"\"\"\n",
    "        print(\"Extracting keywords using TF-IDF...\")\n",
    "        bank_keywords = {}\n",
    "        for bank in df['bank'].unique():\n",
    "            print(f\"  - Processing for {bank}\")\n",
    "            bank_df = df[df['bank'] == bank]\n",
    "            processed_reviews = bank_df[text_column].apply(self.preprocess_text)\n",
    "            \n",
    "            tfidf_matrix = self.vectorizer.fit_transform(processed_reviews)\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            bank_keywords[bank] = feature_names\n",
    "        print(\"Keyword extraction complete.\")\n",
    "        return bank_keywords\n",
    "\n",
    "    def assign_themes(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Assigns predefined themes based on keyword matching.\n",
    "        This is a rule-based approach as suggested by the challenge.\n",
    "        \"\"\"\n",
    "        print(\"Assigning themes...\")\n",
    "        # Define keywords for each theme\n",
    "        theme_map = {\n",
    "            'Account & Login': ['login', 'account', 'password', 'register', 'signin', 'otp'],\n",
    "            'Transactions & Transfers': ['transfer', 'transaction', 'payment', 'send', 'money', 'slow', 'fast', 'fee'],\n",
    "            'UI & Experience': ['ui', 'interface', 'design', 'easy', 'simple', 'update', 'dark mode'],\n",
    "            'Bugs & Performance': ['bug', 'crash', 'error', 'slow', 'performance', 'stuck', 'fix', 'issue'],\n",
    "            'Features & Services': ['feature', 'service', 'loan', 'statement', 'fingerprint', 'biometric']\n",
    "        }\n",
    "\n",
    "        def find_theme(review_text):\n",
    "            review_text = review_text.lower()\n",
    "            found_themes = []\n",
    "            for theme, keywords in theme_map.items():\n",
    "                if any(keyword in review_text for keyword in keywords):\n",
    "                    found_themes.append(theme)\n",
    "            return \", \".join(found_themes) if found_themes else 'General Feedback'\n",
    "\n",
    "        df['themes'] = df[text_column].apply(find_theme)\n",
    "        print(\"Theme assignment complete.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data from task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../data/cleaned_play_store_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "df_sentiment = sentiment_analyzer.analyze(df_cleaned.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "thematic_analyzer = ThematicAnalyzer()\n",
    "df_final = thematic_analyzer.assign_themes(df_sentiment.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top Keywords Per Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "keywords_per_bank = thematic_analyzer.extract_keywords(df_final.copy())\n",
    "for bank, keywords in keywords_per_bank.items():\n",
    "    print(f\"\\nTop keywords for {bank}: {', '.join(keywords[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_final.reset_index(inplace=True)\n",
    "df_final.rename(columns={\n",
    "    'index': 'review_id',\n",
    "    'review': 'review_text',\n",
    "    'themes': 'identified_theme(s)'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select and reorder columns\n",
    "output_columns = ['review_id', 'review_text', 'sentiment_label', 'sentiment_score', 'identified_theme(s)', 'rating', 'date', 'bank']\n",
    "df_output = df_final[output_columns]\n",
    "\n",
    "print(\"\\nFinal DataFrame Head:\")\n",
    "print(df_output.head())\n",
    "\n",
    "# Save the results to a new CSV file \n",
    "output_path = '../data/analyzed_reviews.csv'\n",
    "df_output.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nAnalyzed data with sentiment and themes saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
