{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdulkerima\\IdeaProjects\\customer-feedback-analytics\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform sentiment analysis using a Hugging Face model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        \"\"\"\n",
    "        Initializes the sentiment analysis pipeline.\n",
    "        The challenge recommends this model.\n",
    "        \"\"\"\n",
    "        print(f\"Loading sentiment model: {model_name}...\")\n",
    "        # Using device=0 will use GPU if available, -1 for CPU\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=-1)\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "    def analyze(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Applies sentiment analysis to a DataFrame column.\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "            text_column (str): The column containing text to analyze.\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with added 'sentiment_label' and 'sentiment_score' columns.\n",
    "        \"\"\"\n",
    "        print(\"Applying sentiment analysis...\")\n",
    "        # The pipeline returns a list of dictionaries like {'label': 'POSITIVE', 'score': 0.999}\n",
    "        # We need to handle potential long texts by truncating them for the model\n",
    "        sentiments = self.sentiment_pipeline(df[text_column].fillna('').tolist(), truncation=True)\n",
    "        \n",
    "        # Extract labels and scores\n",
    "        df['sentiment_label'] = [s['label'] for s in sentiments]\n",
    "        df['sentiment_score'] = [s['score'] for s in sentiments]\n",
    "        print(\"Sentiment analysis complete.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThematicAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThematicAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform thematic analysis using NLP techniques.\n",
    "    \"\"\"\n",
    "    def __init__(self, stop_words='english'):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 3), stop_words=stop_words)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Lemmatizes text and removes stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        doc = nlp(text.lower())\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    def extract_keywords(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Extracts top keywords for each bank using TF-IDF.\n",
    "        \"\"\"\n",
    "        print(\"Extracting keywords using TF-IDF...\")\n",
    "        bank_keywords = {}\n",
    "        for bank in df['bank'].unique():\n",
    "            print(f\"  - Processing for {bank}\")\n",
    "            bank_df = df[df['bank'] == bank]\n",
    "            processed_reviews = bank_df[text_column].apply(self.preprocess_text)\n",
    "            \n",
    "            tfidf_matrix = self.vectorizer.fit_transform(processed_reviews)\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            bank_keywords[bank] = feature_names\n",
    "        print(\"Keyword extraction complete.\")\n",
    "        return bank_keywords\n",
    "\n",
    "    def assign_themes(self, df, text_column='review'):\n",
    "        \"\"\"\n",
    "        Assigns predefined themes based on keyword matching.\n",
    "        This is a rule-based approach as suggested by the challenge.\n",
    "        \"\"\"\n",
    "        print(\"Assigning themes...\")\n",
    "        # Define keywords for each theme\n",
    "        theme_map = {\n",
    "            'Account & Login': ['login', 'account', 'password', 'register', 'signin', 'otp'],\n",
    "            'Transactions & Transfers': ['transfer', 'transaction', 'payment', 'send', 'money', 'slow', 'fast', 'fee'],\n",
    "            'UI & Experience': ['ui', 'interface', 'design', 'easy', 'simple', 'update', 'dark mode'],\n",
    "            'Bugs & Performance': ['bug', 'crash', 'error', 'slow', 'performance', 'stuck', 'fix', 'issue'],\n",
    "            'Features & Services': ['feature', 'service', 'loan', 'statement', 'fingerprint', 'biometric']\n",
    "        }\n",
    "\n",
    "        def find_theme(review_text):\n",
    "            review_text = review_text.lower()\n",
    "            found_themes = []\n",
    "            for theme, keywords in theme_map.items():\n",
    "                if any(keyword in review_text for keyword in keywords):\n",
    "                    found_themes.append(theme)\n",
    "            return \", \".join(found_themes) if found_themes else 'General Feedback'\n",
    "\n",
    "        df['themes'] = df[text_column].apply(find_theme)\n",
    "        print(\"Theme assignment complete.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data from task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../data/cleaned_play_store_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment model: distilbert-base-uncased-finetuned-sst-2-english...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdulkerima\\IdeaProjects\\customer-feedback-analytics\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abdulkerima\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Applying sentiment analysis...\n",
      "Sentiment analysis complete.\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "df_sentiment = sentiment_analyzer.analyze(df_cleaned.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning themes...\n",
      "Theme assignment complete.\n"
     ]
    }
   ],
   "source": [
    "thematic_analyzer = ThematicAnalyzer()\n",
    "df_final = thematic_analyzer.assign_themes(df_sentiment.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top Keywords Per Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting keywords using TF-IDF...\n",
      "  - Processing for CBE\n",
      "  - Processing for BOA\n",
      "  - Processing for DASHEN\n",
      "Keyword extraction complete.\n",
      "\n",
      "Top keywords for CBE: access, account, add, allow, amazing, amazing app, app, app like, application, bad\n",
      "\n",
      "Top keywords for BOA: access, account, amazing, android, app, app crash, app work, application, ask, automatically\n",
      "\n",
      "Top keywords for DASHEN: able, account, ahead, amazing, amazing app, amole, app, application, bank, bank super\n"
     ]
    }
   ],
   "source": [
    "keywords_per_bank = thematic_analyzer.extract_keywords(df_final.copy())\n",
    "for bank, keywords in keywords_per_bank.items():\n",
    "    print(f\"\\nTop keywords for {bank}: {', '.join(keywords[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame Head:\n",
      "   review_id                                        review_text  \\\n",
      "0          0                         So bad now and hard to use   \n",
      "1          1  it is so amazing app. but, it is better to upd...   \n",
      "2          2                                         v.good app   \n",
      "3          3                                      very good app   \n",
      "4          4           Very amazing app indeed. I'm enjoying it   \n",
      "\n",
      "  sentiment_label  sentiment_score  \\\n",
      "0        NEGATIVE         0.999806   \n",
      "1        POSITIVE         0.949643   \n",
      "2        POSITIVE         0.995270   \n",
      "3        POSITIVE         0.999868   \n",
      "4        POSITIVE         0.999882   \n",
      "\n",
      "                                 identified_theme(s)  rating        date bank  \n",
      "0                                   General Feedback       5  2025-06-09  CBE  \n",
      "1  Transactions & Transfers, UI & Experience, Fea...       5  2025-06-09  CBE  \n",
      "2                                   General Feedback       4  2025-06-09  CBE  \n",
      "3                                   General Feedback       1  2025-06-09  CBE  \n",
      "4                                   General Feedback       5  2025-06-08  CBE  \n",
      "\n",
      "Analyzed data with sentiment and themes saved to ../data/analyzed_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "df_final.reset_index(inplace=True)\n",
    "df_final.rename(columns={\n",
    "    'index': 'review_id',\n",
    "    'review': 'review_text',\n",
    "    'themes': 'identified_theme(s)'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select and reorder columns\n",
    "output_columns = ['review_id', 'review_text', 'sentiment_label', 'sentiment_score', 'identified_theme(s)', 'rating', 'date', 'bank']\n",
    "df_output = df_final[output_columns]\n",
    "\n",
    "print(\"\\nFinal DataFrame Head:\")\n",
    "print(df_output.head())\n",
    "\n",
    "# Save the results to a new CSV file \n",
    "output_path = '../data/analyzed_reviews.csv'\n",
    "df_output.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\nAnalyzed data with sentiment and themes saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
